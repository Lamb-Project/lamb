# lamb sample .env file 
# Server Configuration change the token!
PIPELINES_HOST=http://localhost:9099
PIPELINES_BEARER_TOKEN=0p3n-w3bu!

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1

# OpenAI Configuration
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-5-mini
OPENAI_MODELS=gpt-5-mini,gpt-5

# Openwebui Authentication
OWI_PATH="/opt/lamb-project/lamb/open-webui/backend/data"
OWI_BASE_URL="http://localhost:8080"

# OpenWebUI Admin Configuration
OWI_ADMIN_EMAIL=admin@owi.com
OWI_ADMIN_NAME=Admin User
OWI_ADMIN_PASSWORD=admin

# Logging Configuration (optional)
LOG_LEVEL=INFO

# Database Configuration
LAMB_DB_PATH= /opt/lamb_v4/code/lamb_v4
LAMB_KB_SERVER= http://localhost:9090 
LAMB_KB_SERVER_TOKEN=0p3n-w3bu! 
LAMB_DB_PREFIX=LAMB_

# Note: LAMB_DB_PREFIX is optional and can be left empty if no prefix is needed

# Signup Configuration
SIGNUP_ENABLED=true
SIGNUP_SECRET_KEY=pepino-secret-key

# LLM CLI Configuration
LLM_DEFAULT_MODEL=gpt-5-mini

# DEV MODE Enables Dev Admin monstrosity 
DEV_MODE=true

TIMELOG_LEVEL=2
TIMELOG_FILE=timelog.md 
