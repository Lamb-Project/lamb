# LAMB Backend Environment Configuration
# This file should be copied to .env and customized for your deployment
# For Docker Compose deployments, paths should match the container paths

# ============================================================================
# LAMB HOST CONFIGURATION (NEW)
# ============================================================================

# LAMB_WEB_HOST: External/public URL for browser-side requests
# This URL is used by the frontend and for generating URLs that browsers need to access
# - Docker Compose (dev): http://localhost:9099 (exposed port)
# - Production: https://lamb.yourdomain.com (your public-facing domain)
LAMB_WEB_HOST=http://localhost:9099

# LAMB_BACKEND_HOST: Internal loopback URL for server-side requests
# Used for internal API calls from Creator Interface to LAMB Core (same container)
# - Docker Compose: http://localhost:9099 (internal loopback)
# - Production: http://localhost:9099 or http://127.0.0.1:9099
LAMB_BACKEND_HOST=http://backend:9099

# PIPELINES_HOST: (DEPRECATED - use LAMB_WEB_HOST and LAMB_BACKEND_HOST instead)
# Kept for backward compatibility. If set, LAMB_WEB_HOST uses this as fallback
# PIPELINES_HOST=http://localhost:9099

# ============================================================================
# AUTHENTICATION & SECURITY
# ============================================================================

# Bearer token for API authentication - CHANGE THIS IN PRODUCTION!
#PIPELINES_BEARER_TOKEN=0p3n-w3bu!
LAMB_BEARER_TOKEN=0p3n-w3bu!


# Signup Configuration
SIGNUP_ENABLED=true
SIGNUP_SECRET_KEY=pepino-secret-key

# LTI Configuration
LTI_SECRET=lamb-lti-secret-key-2024

# ============================================================================
# DATABASE CONFIGURATION
# ============================================================================

# LAMB Database Path - where the LAMB SQLite database is stored
LAMB_DB_PATH=/opt/lamb

# LAMB Database Prefix (optional - can be left empty)
LAMB_DB_PREFIX=LAMB_

# ============================================================================
# OPEN WEBUI INTEGRATION
# ============================================================================

# Path to Open WebUI data directory
OWI_PATH=/opt/lamb/open-webui/backend/data

# OpenWebUI Base URL (internal container communication)
# In Docker Compose: use service name 'openwebui' or localhost
# For local dev without Docker: http://localhost:8080
OWI_BASE_URL=http://openwebui:8080

# OpenWebUI Admin Configuration (for initial setup)
OWI_ADMIN_EMAIL=admin@owi.com
OWI_ADMIN_NAME=Admin User
OWI_ADMIN_PASSWORD=admin

# ============================================================================
# KNOWLEDGE BASE SERVER
# ============================================================================

# LAMB Knowledge Base Server URL
# Docker Compose: http://kb:9090 (using service name) or http://localhost:9090
LAMB_KB_SERVER=http://localhost:9090

# Knowledge Base Server Authentication Token
LAMB_KB_SERVER_TOKEN=0p3n-w3bu!

# ============================================================================
# LLM CONFIGURATION
# ============================================================================

# --- Ollama Configuration ---
# For Docker Compose on Mac/Windows: use host.docker.internal
# For Docker Compose on Linux: use host IP or service name
# For local dev: http://localhost:11434
OLLAMA_BASE_URL=http://host.docker.internal:11434
OLLAMA_MODEL=nomic-embed-text

# Note: Some models don't support embeddings
# Test with: curl -X POST http://OLLAMA:11434/api/embeddings -H "Content-Type: application/json" -d '{"model": "MODEL_NAME", "prompt": "test"}'

# --- OpenAI Configuration ---
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4o-mini
OPENAI_MODELS=gpt-4o-mini,gpt-4o

# --- LLM CLI Configuration ---
LLM_DEFAULT_MODEL=gpt-4o-mini

# ============================================================================
# DEVELOPMENT & DEBUGGING
# ============================================================================

# Development Mode - enables additional debug features
DEV_MODE=true

# Logging Configuration
LOG_LEVEL=INFO

# Time Logging
TIMELOG_LEVEL=2
TIMELOG_FILE=timelog.md

# ============================================================================
# DOCKER COMPOSE NOTES
# ============================================================================
#
# Service URLs for inter-container communication:
# - Backend (this service): http://backend:9099 or http://localhost:9099
# - OpenWebUI: http://openwebui:8080 or http://localhost:8080
# - KB Server: http://kb:9090 or http://localhost:9090
#
# When running with Docker Compose:
# - Use 'localhost' for services in the same container
# - Use service names (e.g., 'openwebui', 'kb') for cross-container communication
# - Use 'host.docker.internal' to access host machine services (Mac/Windows)
# - Exposed ports (9099, 8080, 9090) are accessible from host as localhost:PORT
#
# Project Path: /opt/lamb (set via LAMB_PROJECT_PATH env var in docker-compose.yaml)