# LAMB Next Docker Compose Environment Configuration
# Copy this file to .env (same directory as docker-compose.next.yaml)
# and customize values for your deployment.

# ============================================================================
# LAMB HOST CONFIGURATION
# ============================================================================

# Public URL used by browser-side requests.
# - Local compose: http://localhost:9099
# - Production: https://lamb.yourdomain.com
LAMB_WEB_HOST=http://localhost:9099

# Internal URL used by server-side calls.
# Keep localhost for single-container backend/frontend service.
LAMB_BACKEND_HOST=http://localhost:9099

# ============================================================================
# NETWORK / PORTS
# ============================================================================

LAMB_PORT=9099
KB_PORT=9090
OPENWEBUI_PORT=8080

# ============================================================================
# DATABASE PATHS (inside containers)
# ============================================================================

# LAMB SQLite DB path inside lamb container (mapped to lamb-data volume)
LAMB_DB_PATH=/data/lamb

# Open WebUI data path as seen by lamb container (read-only mount)
OWI_PATH=/data/openwebui

# ============================================================================
# OPEN WEBUI INTEGRATION
# ============================================================================

# Internal Open WebUI URL for service-to-service calls
OWI_BASE_URL=http://openwebui:8080

# Public Open WebUI URL for browser redirects/callbacks
OWI_PUBLIC_BASE_URL=http://localhost:8080

WEBUI_AUTH_TRUSTED_EMAIL_HEADER=X-User-Email
WEBUI_AUTH_TRUSTED_NAME_HEADER=X-User-Name
DEFAULT_USER_ROLE=user

# Optional OpenWebUI secret key (recommended in production)
WEBUI_SECRET_KEY=

# ============================================================================
# AUTHENTICATION & SECURITY
# ============================================================================

# CHANGE THESE IN PRODUCTION
LAMB_BEARER_TOKEN=change-me
LAMB_KB_SERVER_TOKEN=change-me
LAMB_API_KEY=change-me

# Optional settings carried from backend defaults
SIGNUP_ENABLED=true
SIGNUP_SECRET_KEY=pepino-secret-key
LTI_SECRET=lamb-lti-secret-key-2024

# ============================================================================
# KNOWLEDGE BASE SERVER
# ============================================================================

# Internal KB URL consumed by lamb service
LAMB_KB_SERVER=http://kb:9090

# Public/base KB URL used by KB service
KB_HOME_URL=http://localhost:9090

# Default embeddings model settings for KB
EMBEDDINGS_MODEL=nomic-embed-text
EMBEDDINGS_VENDOR=ollama
EMBEDDINGS_ENDPOINT=http://host.docker.internal:11434/api/embeddings
EMBEDDINGS_APIKEY=

# Optional Firecrawl integration
FIRECRAWL_API_URL=http://host.docker.internal:3002
FIRECRAWL_API_KEY=

# ============================================================================
# LLM PROVIDERS
# ============================================================================

# OpenAI
OPENAI_API_KEY=
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4o-mini
OPENAI_MODELS=gpt-4o-mini,gpt-4o

# Ollama (local or remote)
OLLAMA_BASE_URL=http://host.docker.internal:11434
OLLAMA_MODEL=nomic-embed-text

# ============================================================================
# RUNTIME / LOGGING
# ============================================================================

DEV_MODE=false
GLOBAL_LOG_LEVEL=WARNING
