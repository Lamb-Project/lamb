services:
  openwebui:
    image: python:3.11-slim
    container_name: lamb-openwebui
    working_dir: ${LAMB_PROJECT_PATH}/open-webui/backend
    environment:
      - PORT=8080
      - WEBUI_AUTH_TRUSTED_EMAIL_HEADER=X-User-Email
      - WEBUI_AUTH_TRUSTED_NAME_HEADER=X-User-Name
      - PIP_DISABLE_PIP_VERSION_CHECK=1
      - PYTHONDONTWRITEBYTECODE=1
      - PYTHONUNBUFFERED=1
    volumes:
      - ${LAMB_PROJECT_PATH}:${LAMB_PROJECT_PATH}
    ports:
      - "8080:8080"
    depends_on:
      - openwebui-build
    command: >
      sh -lc "echo 'Build folder found, starting backend server...' && \
      cd ${LAMB_PROJECT_PATH}/open-webui/backend && \
      python -m pip install --upgrade pip && \
      pip install -r requirements.txt && \
      mkdir -p data && \
      uvicorn open_webui.main:app --port $$PORT --host 0.0.0.0 --forwarded-allow-ips '*' --reload"

  openwebui-build:
    image: node:20-alpine
    container_name: lamb-openwebui-build
    working_dir: ${LAMB_PROJECT_PATH}/open-webui
    volumes:
      - ${LAMB_PROJECT_PATH}:${LAMB_PROJECT_PATH}
    environment:
      - NODE_OPTIONS=--max-old-space-size=4096
    command: >
      sh -lc "if [ ! -d ${LAMB_PROJECT_PATH}/open-webui/build ] || [ -z \"$(ls -A ${LAMB_PROJECT_PATH}/open-webui/build 2>/dev/null)\" ]; then npm install && npm run build; else echo 'Build folder already exists, skipping build.'; fi"
    # This service only runs the build step and exits

  frontend-build:
    image: node:20-alpine
    container_name: lamb-frontend-build
    working_dir: ${LAMB_PROJECT_PATH}/frontend/svelte-app
    volumes:
      - ${LAMB_PROJECT_PATH}:${LAMB_PROJECT_PATH}
    command: >
      sh -lc "npm install && npm run build"
    # This service only runs the build step and exits

  kb:
    image: python:3.11-slim
    container_name: lamb-kb-server
    working_dir: ${LAMB_PROJECT_PATH}/lamb-kb-server-stable/backend
    environment:
      - PIP_DISABLE_PIP_VERSION_CHECK=1
      - PYTHONDONTWRITEBYTECODE=1
      - PYTHONUNBUFFERED=1
    volumes:
      - ${LAMB_PROJECT_PATH}:${LAMB_PROJECT_PATH}
    ports:
      - "9090:9090"
    command: >
      sh -lc "python -m pip install --upgrade pip && \
      pip install -r requirements.txt && \
      mkdir -p static && \
      (test -f .env || cp .env.example .env) && \
      python start.py"

  backend:
    image: python:3.11-slim
    container_name: lamb-backend
    working_dir: ${LAMB_PROJECT_PATH}/backend
    environment:
      - PORT=9099
      - PIP_DISABLE_PIP_VERSION_CHECK=1
      - PYTHONDONTWRITEBYTECODE=1
      - PYTHONUNBUFFERED=1
    env_file:
      - ${LAMB_PROJECT_PATH}/backend/.env
    volumes:
      - ${LAMB_PROJECT_PATH}:${LAMB_PROJECT_PATH}
    ports:
      - "9099:9099"
    depends_on:
      - openwebui
      - kb
      - frontend-build
    command: >
      sh -lc "python -m pip install --upgrade pip && \
      pip install -r requirements.txt && \
      uvicorn main:app --port $$PORT --host 0.0.0.0 --forwarded-allow-ips '*' --reload"

  frontend:
    image: node:20-alpine
    container_name: lamb-frontend
    working_dir: ${LAMB_PROJECT_PATH}/frontend/svelte-app
    environment:
      - HOST=0.0.0.0
      - PROXY_TARGET=http://backend:9099
    volumes:
      - ${LAMB_PROJECT_PATH}:${LAMB_PROJECT_PATH}
    ports:
      - "5173:5173"
    depends_on:
      - backend
      - frontend-build
    command: >
      sh -lc "test -f static/config.js || cp static/config.js.sample static/config.js; \
      npm install; \
      npm run dev -- --host 0.0.0.0"

  caddy:
    image: caddy:2.8
    container_name: lamb-caddy
    restart: unless-stopped
    environment:
      - LAMB_PROJECT_PATH=${LAMB_PROJECT_PATH}
    volumes:
      - ${LAMB_PROJECT_PATH}/Caddyfile:/etc/caddy/Caddyfile:ro
      - ${LAMB_PROJECT_PATH}/frontend/build:/var/www/frontend:ro
      - caddy_data:/data
      - caddy_config:/config
    ports:
      - "80:80"
      - "443:443"
    depends_on:
      backend:
        condition: service_started
      kb:
        condition: service_started
      openwebui:
        condition: service_started
      frontend-build:
        condition: service_completed_successfully

networks:
  default:
    name: lamb

volumes:
  caddy_data:
  caddy_config:
